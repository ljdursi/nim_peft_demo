{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf2fdd38-0bfc-4dc1-8f81-1e4bce6afc75",
   "metadata": {},
   "source": [
    "# NIMs Three Ways - build.nvidia.com, local, and then fine-tuned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e245ae4-ed65-4129-bb3d-d17cbd21192d",
   "metadata": {},
   "source": [
    "The beauty of developing with NVIDIA NIMs is that you can use them anywhere.  You can develop against remote APIs, then stand them up locally, and then use the single NIM to serve multiple, parameter-efficient fine-tuned models. \n",
    "\n",
    "Here we'll walk through a simple example, with Llama-3.1 8B Instruct.   Let's get started!\n",
    "\n",
    "<div>\n",
    "<img src=\"https://developer.download.nvidia.com/images/products/practitioner-nim-1920x1080.png\" width=\"50%\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6dabf1-c7c1-4431-8587-275ea86b38bb",
   "metadata": {},
   "source": [
    "## Set NGC API Key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f267086-6487-4f2c-97d5-0c20a72fd4d2",
   "metadata": {},
   "source": [
    "We'll need an API key to use the nims - you can get yours (including free credits for developers!) at build.nvidia.com.   I've got mine in a local .env file, we'll use dotenv to read and use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e20c9f-eec8-48c0-b8e6-b7b9f851800b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb12eec-d2b9-4c10-b962-2177b84a2620",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e838bdc3-2ab4-411e-b3b5-b15ba1f23c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "NGC_API_KEY=os.getenv('NGC_API_KEY')\n",
    "NGC_API_KEY_LOCAL=os.getenv('NGC_API_KEY_LOCAL')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44220c4e-7186-4dcb-8c42-3f3bcba2ddfe",
   "metadata": {},
   "source": [
    "## Some Routines for API Calls To NIMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e51f819-0666-4abe-af9b-c6a1a5033562",
   "metadata": {},
   "source": [
    "NIMs use standard OpenAPI interfaces, so we can just use that package.   Then we'll define some routines to do the most simple kind of NIM interaction - a simple completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e978cc0-f3dd-42f6-8249-203c83df18b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731b2634-a910-4d0d-b802-481a7357726d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "def nim_client(url, key=None):\n",
    "    return OpenAI(base_url = url, api_key = key)\n",
    "\n",
    "def simple_nim_complete(client, message_content, model=\"meta/llama-3.1-8b-instruct\", temperature=0.5, top_p=1, max_tokens=1024):\n",
    "    return client.chat.completions.create(model=model,\n",
    "                                          messages=[{\"role\":\"user\",\n",
    "                                                     \"content\": message_content}],\n",
    "                                          temperature=temperature,\n",
    "                                          top_p=top_p,\n",
    "                                          max_tokens=max_tokens,\n",
    "                                          stream=True)\n",
    "def print_completion(completion):\n",
    "    for chunk in completion:\n",
    "      if chunk.choices[0].delta.content is not None:\n",
    "        print(chunk.choices[0].delta.content, end=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42f5da9-3d9b-4678-b3c3-d6864ea7b511",
   "metadata": {},
   "source": [
    "## Develop on build.nvidia.com\n",
    "\n",
    "Let's create a remote client and ask our first simple question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7eb4e3b-d175-4082-979a-b507a5d681a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_client = nim_client(\"https://integrate.api.nvidia.com/v1\", key=NGC_API_KEY)\n",
    "completion = simple_nim_complete(remote_client, \"What is your name?\", model=\"nvdev/meta/llama-3.1-8b-instruct\")\n",
    "\n",
    "print_completion(completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897f5ec7-8912-42f9-8d75-7590a5af4a79",
   "metadata": {},
   "source": [
    "Now let's try something harder - a math question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1855490-f6ce-4a35-9ba2-9d880d3c9d8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "completion = simple_nim_complete(remote_client, \"\"\"A parabola with equation $y=xˆ2+bx+c$ passes through \n",
    "                                                   the points $(−1,−11)$ and $(3 ,17)$. \n",
    "                                                   What is $c$?\"\"\", model=\"nvdev/meta/llama-3.1-8b-instruct\")\n",
    "\n",
    "print_completion(completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deef777-df13-4d6c-8734-b3d0c35dfb0f",
   "metadata": {},
   "source": [
    "## Install Locally\n",
    "### Set up NIM cache directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0d6d4f-d92b-4d17-b8bd-a911908dfa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "nim_cache_path=\"/tmp/nim/.cache\"\n",
    "os.environ[\"NIM_CACHE_PATH\"] = nim_cache_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2451fa5-1c0b-4c41-b599-b3df3fc57bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $NIM_CACHE_PATH\n",
    "!chmod -R 777 $NIM_CACHE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2c31b5-9fae-4a8b-8234-fe7f8d9971dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "repository = \"nim/meta/llama-3.1-8b-instruct\"\n",
    "os.environ[\"CONTAINER_NAME\"] = \"Llama-3.1-8B-Instruct\"\n",
    "os.environ[\"IMG_NAME\"]= f\"nvcr.io/{repository}:1.3.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3c82ac-81cb-4cc3-8b6b-31e725a1dfc8",
   "metadata": {},
   "source": [
    "### Run base NIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc134a5-e87a-45ad-a5de-fa428ecc3289",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!docker run -d --rm --name=$CONTAINER_NAME \\\n",
    "  --gpus all \\\n",
    "  --shm-size=32GB \\\n",
    "  --network=host \\\n",
    "  -e NGC_API_KEY=$NGC_API_KEY \\\n",
    "  -v \"$NIM_CACHE_PATH:/opt/nim/.cache\" \\\n",
    "  -u $(id -u) \\\n",
    "  $IMG_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4fff45-a15b-4cc9-b827-f13bd4f05baa",
   "metadata": {},
   "source": [
    "Let's watch the NIM start up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc108cf-9bdb-459c-8d2e-4244268dc785",
   "metadata": {},
   "outputs": [],
   "source": [
    "nim_docker_id = \"d98f79e6e77812ba8e8d36719061f8b09f0d48fbb849def6c04732320c9c3b03\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b90df7-52a1-4e7c-99b4-a72fa79c5b30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!docker logs -f {nim_docker_id}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b43c18-0d44-45b9-a432-12fc84e37118",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "We can query our new endpoint to see what models are available..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea3664f-6094-4536-912d-82f052a035c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9575a98-a592-481e-9ad4-b11619b06e7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "r = requests.get('http://0.0.0.0:8000/v1/models')\n",
    "print(f\"{r.status_code=}\")\n",
    "print(\"Models available = \", [item['id'] for item in r.json()['data']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bf8d76-ed88-4908-b2bc-88acb1624769",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Run against the local NIM\n",
    "\n",
    "Fantastic!  Now let's try those API calls again..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f19dba4-ecbd-499f-a6c9-a77337e31749",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "local_client = nim_client(\"http://localhost:8000/v1\", key=NGC_API_KEY_LOCAL)\n",
    "completion = simple_nim_complete(local_client, \"What is your name?\")\n",
    "\n",
    "print_completion(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d3591c-ca94-4004-8d52-e035cd087372",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = simple_nim_complete(local_client, \"\"\"A parabola with equation $y=xˆ2+bx+c$ passes through \n",
    "                                                   the points $(−1,−11)$ and $(3 ,17)$. \n",
    "                                                   What is $c$?\"\"\")\n",
    "\n",
    "print_completion(completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ce7782-fbab-4776-b31d-3d0e6efc0064",
   "metadata": {},
   "source": [
    "## Use a local fine-tuned model\n",
    "\n",
    "Now that we've got the model running, let's stop the NIM and add a fine-tuned model.  We're going to add a fine-tuned model from hugging face, and for that we're going to need Git LFS installed...\n",
    "\n",
    "### Set up Git LFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3526cd-9500-4dfa-827a-9adb76121199",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!sudo apt-get install git-lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3658a4b-4908-48a5-ae60-d7f1206c888e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git lfs install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42f6d92-ec26-4649-a671-2dafe3bcd074",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://huggingface.co/nvidia/OpenMath2-Llama3.1-8B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7a2733-c36c-46b6-8ea9-46a63a0e2450",
   "metadata": {},
   "source": [
    "### Start NIM with the new Fine-tuned model\n",
    "\n",
    "We'll stop the currently running image and point the new container to a different model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda11850-f8f9-4a15-bd4d-edc87c8d22dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker stop {nim_docker_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70861890-1bd3-4784-961a-c262dd142f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -d --rm --name=\"$CONTAINER_NAME\"_PEFT \\\n",
    "    --gpus all \\\n",
    "    --shm-size=32GB \\\n",
    "    --network=host \\\n",
    "    -e NGC_API_KEY=$NGC_API_KEY \\\n",
    "    -u $(id -u) \\\n",
    "    -e NIM_FT_MODEL=/opt/weights/hf/OpenMath2-Llama3.1-8B \\\n",
    "    -e NIM_SERVED_MODEL_NAME=OpenMath2-Llama3.1-8B \\\n",
    "    -v \"$NIM_CACHE_PATH:/opt/nim/.cache\" \\\n",
    "    -v ${PWD}:/opt/weights/hf \\\n",
    "    $IMG_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61e1343-075f-41ed-abb1-e20229316586",
   "metadata": {},
   "outputs": [],
   "source": [
    "nim_docker_id=\"379dffcf4a26ccdd32e6ace11c25ed4239d0db54015a1cd663b8c4c7c0e27383\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbafaf2-16fb-4dba-bd63-d1ed18806ac9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!docker logs -f {nim_docker_id}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2961f9d8-2a75-4d6d-8d4f-a671b0a17417",
   "metadata": {},
   "source": [
    "Now let's query the API and see what models are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce82cf4b-3fd8-4603-9043-599d47b3d317",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('http://0.0.0.0:8000/v1/models')\n",
    "print(f\"{r.status_code=}\")\n",
    "print(\"Models available = \", [item['id'] for item in r.json()['data']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f07f3a-e45a-4ab0-a320-d8579273f333",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = simple_nim_complete(local_client, \"What is your name?\", model='OpenMath2-Llama3.1-8B')\n",
    "\n",
    "print_completion(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298ff07d-a5f8-446c-b810-80dc7d2ea8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = simple_nim_complete(local_client, \"\"\"A parabola with equation $y=xˆ2+bx+c$ passes through \n",
    "                                                   the points $(−1,−11)$ and $(3 ,17)$. \n",
    "                                                   What is $c$?\"\"\",\n",
    "                                               model='OpenMath2-Llama3.1-8B')\n",
    "\n",
    "print_completion(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8492f7-a9b9-4609-9af5-41143ae5519e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker stop {nim_docker_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc31908-8fe8-40dc-8849-40f6bf9d215d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
